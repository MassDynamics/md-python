{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from IPython.display import JSON, display\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from md_python import MDClient, Experiment\n",
    "\n",
    "\n",
    "\n",
    "client = MDClient()\n",
    "health = client.health.check()\n",
    "\n",
    "# Run and minimally test\n",
    "assert isinstance(health, dict)\n",
    "display(JSON(health, expanded=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Metadata files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/Users/giuseppeinfusini/wd/Data_for_upload_md/MD-format/Small_data\"\n",
    "experiment_design_filename = \"experiment_design_COMBINED.csv\"\n",
    "sample_metadata_filename = \"sample_metadata_COMBINED.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from md_python.models import ExperimentDesign, SampleMetadata\n",
    "\n",
    "def load_experiment_design(dir_path: str, filename: str, delimiter: str = \",\") -> ExperimentDesign:\n",
    "    \"\"\"Load ExperimentDesign from a CSV located at dir_path/filename.\"\"\"\n",
    "    return ExperimentDesign.from_csv(os.path.join(dir_path, filename), delimiter=delimiter)\n",
    "\n",
    "def load_sample_metadata(dir_path: str, filename: str, delimiter: str = \",\") -> SampleMetadata:\n",
    "    return SampleMetadata.from_csv(os.path.join(dir_path, filename), delimiter=delimiter)\n",
    "\n",
    "exp_design = load_experiment_design(metadata_path, experiment_design_filename)\n",
    "sample_metadata = load_sample_metadata(metadata_path, sample_metadata_filename)\n",
    "assert isinstance(exp_design, ExperimentDesign)\n",
    "print(exp_design)  # brief preview via __str__\n",
    "assert isinstance(sample_metadata, SampleMetadata)\n",
    "print(sample_metadata)  # brief preview via __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp = Experiment(\n",
    "    name=\"test_api_client_04\",\n",
    "    source=\"md_format\",\n",
    "    labelling_method=\"lfq\",\n",
    "    s3_bucket=\"md-development-test-data\",\n",
    "    s3_prefix=\"small_drc_api_test/\",\n",
    "    filenames=[\"proteomics_proteins_COMBINED.tsv\", \"proteomics_peptides_COMBINED.tsv\"],\n",
    "    experiment_design=exp_design,\n",
    "    sample_metadata=sample_metadata,\n",
    ")\n",
    "\n",
    "# experiment_id = client.experiments.create(exp)\n",
    "# print(experiment_id)\n",
    "# assert isinstance(experiment_id, str) and len(experiment_id) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp for developpment\n",
    "experiment_id = \"cbd62af2-19da-476d-8bb1-bda6b3823c73\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Wait for experiment to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from md_python import MDClient\n",
    "\n",
    "def wait_for_experiment(client: MDClient, experiment_id: str, poll_s: int = 5, timeout_s: int = 1800):\n",
    "    end = time.monotonic() + timeout_s\n",
    "    last = None\n",
    "    while time.monotonic() < end:\n",
    "        exp = client.experiments.get_by_id(experiment_id)\n",
    "        if exp.status != last:\n",
    "            print(f\"status={exp.status}\")\n",
    "            last = exp.status\n",
    "        if exp.status.upper() in {\"COMPLETED\"}:\n",
    "            return exp\n",
    "        if exp.status.upper() in {\"FAILED\", \"ERROR\", \"CANCELLED\"}:\n",
    "            return exp\n",
    "        time.sleep(poll_s)\n",
    "    raise TimeoutError(f\"Experiment {experiment_id} not completed within {timeout_s}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_exp = wait_for_experiment(client, experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = client.datasets.list_by_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [d for d in datasets if d.name == exp.name][0]\n",
    "str(dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from uuid import UUID\n",
    "from md_python import Dataset\n",
    "from md_python.models import SampleMetadata\n",
    "\n",
    "def create_pairwise_comparison_dataset(\n",
    "    input_dataset_ids: List[str],\n",
    "    dataset_name: str,\n",
    "    sample_metadata: SampleMetadata,\n",
    "    condition_column: str,\n",
    "    condition_comparisons: List[List[str]],\n",
    "    filter_valid_values_logic: str = \"at least one condition\", # [\"all conditions\", \"at least one condition\", \"full experiment\"]\n",
    "    filter_values_criteria: Dict[str, Any] = {\"method\": \"percentage\", \"filter_threshold_percentage\": 0.5}, # 'count', 'filter_threshold_count'\n",
    "    fit_separate_models: bool = True,\n",
    "    limma_trend: bool = True,\n",
    "    robust_empirical_bayes: bool = True,\n",
    "    control_variables: List[Dict[str, str]] = None,\n",
    "    entity_type: str = \"protein\",\n",
    "    job_slug: str = \"pairwise_comparison\",\n",
    ") -> Dataset:\n",
    "    return Dataset(\n",
    "        input_dataset_ids=[UUID(x) for x in input_dataset_ids],\n",
    "        name=dataset_name,\n",
    "        job_slug=job_slug,\n",
    "        job_run_params={\n",
    "            \"condition_column\": condition_column,\n",
    "            \"condition_comparisons\": {\"condition_comparison_pairs\": condition_comparisons},\n",
    "            \"experiment_design\": sample_metadata.to_columns(),\n",
    "            \"filter_valid_values_logic\": filter_valid_values_logic,\n",
    "            \"filter_values_criteria\": filter_values_criteria,\n",
    "            \"fit_separate_models\": fit_separate_models,\n",
    "            \"limma_trend\": limma_trend,\n",
    "            \"robust_empirical_bayes\": robust_empirical_bayes,\n",
    "            \"control_variables\": control_variables,\n",
    "            \"entity_type\": entity_type,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pw_dataset = create_pairwise_comparison_dataset(\n",
    "    input_dataset_ids=[str(dataset.id)],\n",
    "    dataset_name=\"Pairwise test\",\n",
    "    sample_metadata=sample_metadata,\n",
    "    condition_column=\"condition\",\n",
    "    condition_comparisons=[[\"md00001_a\", \"md00001_b\"], [\"md00001_a\", \"md00003_a\"]],\n",
    "    # optional params keep defaults...\n",
    ")\n",
    "dataset_id = client.datasets.create(pw_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"c48bb19c-2e0d-44e2-96c4-54448f2ab2fb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections.abc import Callable\n",
    "\n",
    "def find_non_jsonable_in(obj, path=\"root\"):\n",
    "    bad = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            bad += find_non_jsonable_in(v, f\"{path}.{k}\")\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            bad += find_non_jsonable_in(v, f\"{path}[{i}]\")\n",
    "    else:\n",
    "        try:\n",
    "            json.dumps(obj)\n",
    "        except TypeError:\n",
    "            bad.append((path, type(obj).__name__, repr(obj)))\n",
    "    return bad\n",
    "\n",
    "def debug_dataset_payload(ds):\n",
    "    payload = {\n",
    "        \"dataset\": {\n",
    "            \"input_dataset_ids\": [str(x) for x in ds.input_dataset_ids],\n",
    "            \"name\": ds.name,\n",
    "            \"job_slug\": ds.job_slug,\n",
    "            \"job_run_params\": ds.job_run_params or {},\n",
    "        }\n",
    "    }\n",
    "    problems = find_non_jsonable_in(payload)\n",
    "    print(\"Non-JSONable entries:\", problems)\n",
    "    print(json.dumps(payload, indent=2, default=str)[:2000])\n",
    "    return payload, problems\n",
    "\n",
    "# Use:\n",
    "payload, problems = debug_dataset_payload(pw_dataset)\n",
    "assert not problems, \"Fix these entries (likely a method without parentheses)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"condition_comparison_pairs\": [\n",
    "        [\n",
    "            \"Control\",\n",
    "            \"Stage 1\"\n",
    "        ],\n",
    "        [\n",
    "            \"Control\",\n",
    "            \"Stage 3\"\n",
    "        ],\n",
    "        [\n",
    "            \"Control\",\n",
    "            \"Stage 5\"\n",
    "        ],\n",
    "        [\n",
    "            \"Control\",\n",
    "            \"Outlier\"\n",
    "        ]\n",
    "    ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md-api-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
