{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON, display\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from md_python import MDClient, Experiment\n",
    "from md_python import ExperimentDesign, SampleMetadata\n",
    "\n",
    "from md_python import PairwiseComparisonDataset\n",
    "\n",
    "\n",
    "client = MDClient()\n",
    "health = client.health.check()\n",
    "\n",
    "# Run and minimally test\n",
    "assert isinstance(health, dict)\n",
    "display(JSON(health, expanded=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Metadata files\n",
    "\n",
    "Metadata files must be stored in a local directory.\n",
    "For LFQ experiments, the experiment design and sample metadata can be combined into a single file, provided that the columns \"filename\", \"sample_name\", and \"condition\" are included.\n",
    "\n",
    "The files_to_upload variable is a list of files that have already been uploaded to s3_bucket/s3_key/ and are expected to be picked up by the API. These files typically include the Spectronaut output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/Users/giuseppeinfusini/wd/Data_for_upload_md/MD-format/Small_data\"\n",
    "experiment_design_filename = \"experiment_design_COMBINED.csv\"\n",
    "sample_metadata_filename = \"experiment_design_COMBINED.csv\"\n",
    "\n",
    "# S3 bucket files\n",
    "files_to_upload = [\n",
    "    \"proteomics_proteins_COMBINED.tsv\",\n",
    "    \"proteomics_peptides_COMBINED.tsv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_design(\n",
    "    dir_path: str, filename: str, delimiter: str = \",\"\n",
    ") -> ExperimentDesign:\n",
    "    \"\"\"Load ExperimentDesign from a CSV located at dir_path/filename.\"\"\"\n",
    "    return ExperimentDesign.from_csv(\n",
    "        os.path.join(dir_path, filename), delimiter=delimiter\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample_metadata(\n",
    "    dir_path: str, filename: str, delimiter: str = \",\"\n",
    ") -> SampleMetadata:\n",
    "    return SampleMetadata.from_csv(\n",
    "        os.path.join(dir_path, filename), delimiter=delimiter\n",
    "    )\n",
    "\n",
    "\n",
    "exp_design = load_experiment_design(metadata_path, experiment_design_filename)\n",
    "sample_metadata = load_sample_metadata(metadata_path, sample_metadata_filename)\n",
    "assert isinstance(exp_design, ExperimentDesign)\n",
    "print(exp_design)  # brief preview via __str__\n",
    "assert isinstance(sample_metadata, SampleMetadata)\n",
    "print(sample_metadata)  # brief preview via __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(\n",
    "    name=\"test_api_client_04\",\n",
    "    source=\"md_format\",\n",
    "    labelling_method=\"lfq\",\n",
    "    s3_bucket=\"md-development-test-data\",\n",
    "    s3_prefix=\"small_drc_api_test/\",\n",
    "    filenames=[\"proteomics_proteins_COMBINED.tsv\", \"proteomics_peptides_COMBINED.tsv\"],\n",
    "    experiment_design=exp_design,\n",
    "    sample_metadata=sample_metadata,\n",
    ")\n",
    "\n",
    "# experiment_id = client.experiments.create(exp)\n",
    "# print(experiment_id)\n",
    "# assert isinstance(experiment_id, str) and len(experiment_id) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp for developpment\n",
    "experiment_id = \"cbd62af2-19da-476d-8bb1-bda6b3823c73\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Wait for experiment to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_experiment = client.experiments.wait_until_complete(experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Pairwise comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Find the initial intensity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.datasets.find_initial_dataset(experiment_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# MISSING STEP of imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Define pairwise comparisons by selecting a control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = sample_metadata.pairwise_vs_control(\n",
    "    column=\"condition\", control=\"md00001_a\"\n",
    ")\n",
    "comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PairwiseComparisonDataset(\n",
    "    input_dataset_ids=[str(dataset.id)],\n",
    "    dataset_name=\"Pairwise test full-02\",\n",
    "    sample_metadata=sample_metadata,\n",
    "    condition_column=\"condition\",\n",
    "    condition_comparisons=comparisons,\n",
    ")\n",
    "dataset_id = pw.run(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = client.datasets.wait_until_complete(\n",
    "    experiment_id=experiment_id,\n",
    "    dataset_id=dataset_id,\n",
    ")\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from uuid import UUID\n",
    "from md_python import Dataset\n",
    "from md_python.models import SampleMetadata\n",
    "\n",
    "\n",
    "def create_pairwise_comparison_dataset(\n",
    "    input_dataset_ids: List[str],\n",
    "    dataset_name: str,\n",
    "    sample_metadata: SampleMetadata,\n",
    "    condition_column: str,\n",
    "    condition_comparisons: List[List[str]],\n",
    "    filter_valid_values_logic: str = \"at least one condition\",  # [\"all conditions\", \"at least one condition\", \"full experiment\"]\n",
    "    filter_values_criteria: Dict[str, Any] = {\n",
    "        \"method\": \"percentage\",\n",
    "        \"filter_threshold_percentage\": 0.5,\n",
    "    },  # 'count', 'filter_threshold_count'\n",
    "    fit_separate_models: bool = True,\n",
    "    limma_trend: bool = True,\n",
    "    robust_empirical_bayes: bool = True,\n",
    "    control_variables: List[Dict[str, str]] = None,\n",
    "    entity_type: str = \"protein\",\n",
    "    job_slug: str = \"pairwise_comparison\",\n",
    ") -> Dataset:\n",
    "    return Dataset(\n",
    "        input_dataset_ids=[UUID(x) for x in input_dataset_ids],\n",
    "        name=dataset_name,\n",
    "        job_slug=job_slug,\n",
    "        job_run_params={\n",
    "            \"condition_column\": condition_column,\n",
    "            \"condition_comparisons\": {\n",
    "                \"condition_comparison_pairs\": condition_comparisons\n",
    "            },\n",
    "            \"experiment_design\": sample_metadata.to_columns(),\n",
    "            \"filter_valid_values_logic\": filter_valid_values_logic,\n",
    "            \"filter_values_criteria\": filter_values_criteria,\n",
    "            \"fit_separate_models\": fit_separate_models,\n",
    "            \"limma_trend\": limma_trend,\n",
    "            \"robust_empirical_bayes\": robust_empirical_bayes,\n",
    "            \"control_variables\": control_variables,\n",
    "            \"entity_type\": entity_type,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_dataset = create_pairwise_comparison_dataset(\n",
    "    input_dataset_ids=[str(dataset.id)],\n",
    "    dataset_name=\"Pairwise test\",\n",
    "    sample_metadata=sample_metadata,\n",
    "    condition_column=\"condition\",\n",
    "    condition_comparisons=[[\"md00001_a\", \"md00001_b\"], [\"md00001_a\", \"md00003_a\"]],\n",
    "    # optional params keep defaults...\n",
    ")\n",
    "\n",
    "dataset_id = client.datasets.create(pw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp for develpment\n",
    "dataset_id = \"3b19ae4b-282e-4eb0-9d2b-ec1c0c7a8084\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from typing import Dict, Any, Optional\n",
    "# from md_python import MDClient\n",
    "\n",
    "\n",
    "def wait_for_dataset(\n",
    "    client: MDClient,\n",
    "    experiment_id: str,\n",
    "    dataset_id: str,\n",
    "    poll_s: int = 5,\n",
    "    timeout_s: int = 1800,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Polls list_by_experiment until the dataset.state is terminal.\"\"\"\n",
    "    end = time.monotonic() + timeout_s\n",
    "    last: Optional[str] = None\n",
    "    while time.monotonic() < end:\n",
    "        dds = client.datasets.list_by_experiment(experiment_id=experiment_id)\n",
    "        ds = next((d for d in dds if str(d.id) == dataset_id), None)\n",
    "        if ds:\n",
    "            state = ds.state  # use dataset.state key\n",
    "            if state != last:\n",
    "                print(f\"state={state}\")\n",
    "                last = state\n",
    "            if state in {\"COMPLETED\", \"FAILED\", \"ERROR\", \"CANCELLED\"}:\n",
    "                return ds\n",
    "        else:\n",
    "            if last is None:\n",
    "                print(\"waiting for dataset to appear...\")\n",
    "        time.sleep(poll_s)\n",
    "    raise TimeoutError(f\"Dataset {dataset_id} not terminal within {timeout_s}s\")\n",
    "\n",
    "\n",
    "# tiny test\n",
    "# result = wait_for_dataset(client, experiment_id, dataset_id)\n",
    "# assert result.get(\"state\") in {\"COMPLETED\", \"FAILED\", \"ERROR\", \"CANCELLED\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wait_for_dataset(client, experiment_id, dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md-api-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
